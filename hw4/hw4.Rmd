# STAT 327 Homework 4
We'll grade your homework by opening your "hw4.Rmd" file in RStudio (in a directory containing "farm.csv", "scores.csv", and "hw4freeCode.R"), clicking "Knit HTML", reading the HTML output, and reading your "hw4.Rmd" file. You should write R code anywhere you see an empty R code chunk.

Name: KEHUI YAO

Email: kyao24@wisc.edu

# Part 1: A "jackknife" procedure to find the most outlying point in a linear relationship between two variables

First load the "XML" package to give access to `readHTMLTable()` and the "RCurl" package for access to `getURL()`.
```{r}
library(rvest)
library(tidyverse)
```

Use R to get the land area (sq. miles) of each state from the second table (labeled "Land area") in the web page https://simple.wikipedia.org/wiki/List_of_U.S._states_by_area. Hint: you can use `tables = readHTMLTable(getURL("https://simple.wikipedia.org/wiki/List_of_U.S._states_by_area"))` to read the data. Include code to remove the commas from the numbers.
```{r}
url="https://simple.wikipedia.org/wiki/List_of_U.S._states_by_area"
land.area=url %>% 
  read_html() %>% 
  html_nodes(xpath='//*[@id="mw-content-text"]/div/table[2]') %>% 
  html_table() %>% 
  as.data.frame()
remove_comma=function(x){
  gsub(",","",x)
}
land.area$km2=map(land.area$km2,remove_comma) %>% unlist()
land.area$sq.miles=map(land.area$sq.miles,remove_comma) %>% unlist()  
land.area
```

Use R to get farm areas of states from "farm.csv". (Note: I got the data in 2013 from the spreadsheet in the row labeled "825 - Farms--Number and Acreage by State [Excel 131k] ..." at http://www.census.gov/compendia/statab/cats/agriculture/farms_and_farmland.html. I took the 2010 acreage (1000) column, multiplied by 1000, and divided by 640 (sq. miles per acre). You do not need to use this spreadsheet--just use "farm.csv".)
```{r}
farm.csv=read.csv("farm.csv",header = T)
```

Create a data frame called "area" whose columns are "state", "farm", and "land", which contain state names, farm areas, and land areas, respectively. Hint: the states aren't in the same order in the two data sets, so getting the "area" data frame right requires a little care.
```{r}
area=farm.csv %>% 
  left_join(land.area,by = c("state"="State")) %>% 
  rename(farm=sq.miles.x,land=sq.miles.y) %>% 
  select(state,farm,land) %>% 
  mutate(land=as.numeric(as.character(land)))

```

Make a scatterplot of y = farm area vs. x = land area.
```{r}
scatterplot=ggplot(data=area,aes(x=land,y=farm))+geom_point()
scatterplot
plot(x=area$land,y=area$farm)
#index=c()
#index=c(identify(area$land,area$farm),index)
```

There are two prominent outliers. Use `identify()` to find their indices.

Unfortunately, `identify()` doesn't work on an R graph that we're viewing through an HTML page. We can use the RStudio menu command "Chunks > Run all" to run all the code in this file in the console, and then click on the graph in RStudio's "Plots" tab. Once you know the indices, just assign them to variables so you can use them later. Then comment out your call to identify().
```{r}
index=c(2,43)
area[index,]
```

The two outliers are Texas, which fits the roughly linear trend of the rest of the data, and Alaska, which does not fit.

Make a linear model of y = farm area vs. x = land area. Make your scatterplot again, and this time add the regression line to it. Then make a linear model of the same data, except with Alaska removed. Add that regression line, colored red, to your scatterplot.
```{r}
scatterplot+geom_smooth(method="lm")+geom_smooth(method="lm",data = area[-index[1],],aes(x=land,y=farm),color="red")
```

Notice that, with respect to the original regression line, Texas has the biggest residual (difference in actual and predicted y), because Alaska pulled the line down toward itself. But really Alaska is the outlier! Next we'll do a "jackknife" procedure to discover computationally that Alaska is the most important outlier.

Make a plot of the residuals for the original model. (Hint: they're available in the output of `lm()`.)
```{r}
fit=lm(farm~land,area)
plot(fit)
```

Notice again that the Texas residual is bigger than the Alaska residual.

Next use a loop to create n=50 models. In step i, make a model of the data with observation i removed. Then predict the value of y[i] from that model, and find the residual (difference) between (the removed) y[i] and the prediction. Save these residuals in a vector `r.jack`. (A "jackknife" procedure works by removing one observation (or several) from a data set, and then making a prediction from that smaller data set, and repeating this for each observation.)
```{r}
res=c()
for (i in 1:50){
  fit=lm(farm~land,area[-i,])
  res=c(res,predict.lm(fit,newdata = data.frame(land=area$land[i]))-area$farm[i])
}
```

Plot these "jackknife" residuals.
```{r}
plot(res)
```

Notice now that Alaska is clearly the real outlier.

# Part 2: ggplot2 graphics

Use `ggplot2` to solve make several graphs. First, here's code to load, or install and load, the package.
```{r}
if (!require("ggplot2")) {
  install.packages("ggplot2")
  stopifnot(require("ggplot2"))
  }
```

1. Consider the built-in data set `warpbreaks`. (See `?warpbreaks`, http://en.wikipedia.org/wiki/Warp_%28weaving%29, and http://en.wikipedia.org/wiki/Power_loom#Operation.) Make a histogram of the numbers of warp breaks.

```{r}
ggplot(data=warpbreaks, aes(breaks)) + 
  geom_histogram(col="red", 
                 fill="green", 
                 alpha = .2) + 
  labs(title="Histogram for breaks") +
  labs(x="breaks", y="Count") 

```

2. Make a density plot of the numbers of warp breaks.
```{r}
ggplot(warpbreaks, aes(breaks)) +
  geom_density()
```

3. Make two density plots of warp breaks, using a different color for each wool type, on a single panel. Does the wool type have a strong effect on the number of breaks?
```{r}
ggplot(warpbreaks, aes(breaks)) +
  geom_density(aes(col=wool))
```
Yes, the type A wool correspond to more breaks.


4. Make three density plots of warp breaks, using a different color for each tension level, on a single panel. How does tension seem to affect the number of breaks?
```{r}
ggplot(warpbreaks, aes(breaks)) +
  geom_density(aes(col=tension))
```
Lighter tension correspond to more breaks.



5. "Old Faithful" is a geyser in Yellowstone National Park that erupts on a remarkably regular schedule (http://en.wikipedia.org/wiki/Old_Faithful). Make a scatterplot of waiting time ($y$) vs. most recent eruption time ($x)$ from the built-in `faithful` data set. (See `?faithful`.) Include a simple linear regression line. What is the most striking feature of this plot?
```{r}
ggplot(data=faithful,aes(x=eruptions,y=waiting))+geom_point()+geom_smooth(method="lm")
```
They have linear relationship.


# Part 3: Web-scraping

prepare some functions
```{r}
### clean moive url
get_movie_url=function(x){
  sub("(\\/.*\\/)?\\?.*","\\1",x)
}

### clean moive's title and year
get_moive_info=function(x){
  y=gsub("(\n| )","",x)
  sub(".*?\\.(.*)\\((.*)\\)","\\1 \\2",y)
}

### make the x vector the same length as the sum of index
expand_x=function(x,index){
  len=length(x)
  expand.x=c()
  for (i in 1:len){
    expand.x=c(expand.x,rep(x[i],index[i]))
  }
  expand.x
}

### clean cast information
shrink_cast=function(x){
  index=which(x=="Rest of cast listed alphabetically:")
  if (sum(x=="Rest of cast listed alphabetically:")!=0) return(x[1:index-1])
  return(x)
}

```

get the data
```{r}
url="http://www.imdb.com/chart/top"

### general information about top 250 movies, from which we extract title and year
movie.info=
  url %>% 
  read_html() %>% 
  html_nodes(".titleColumn") %>% 
  html_text()

### top.250 collects the title and year of the top 250 ratings movie
top.250=map(movie.info,get_moive_info) %>% 
  unlist() %>% 
  strsplit(" ") 
top.250=do.call(rbind,top.250)

### full credit movie's url
movie.url=
  url %>% 
  read_html() %>% 
  html_nodes(".titleColumn") %>% 
  html_children() %>% 
  html_attr("href")
movie.url=matrix(movie.url,byrow=T,ncol=2) %>% 
  .[,1] %>% 
  map(get_movie_url) %>% 
  unlist()
movie.url=paste("http://www.imdb.com",movie.url,"fullcredits",sep="")


### producer information
producer.info=list()
for (i in 1:length(movie.url)){
  detail.url=movie.url[i]
  producer.info[[i]]=
    detail.url %>% 
    read_html() %>% 
    html_node(xpath='//*[@id="fullcredits_content"]/table[4]') %>% 
    html_table() %>% 
    .[,1]
}

producer.info %>% 
  unlist %>% 
  table() %>% 
  sort(decreasing = T) %>% 
  .[1:5]
  

```

# Part 4: Extra Credit (not required; worth 0, 1, or 2 points)

* Collect Year, Director, Rating, Number of Votes and Cast (first billed only)

```{r}
### director information
director.info=list()
for (i in 1:length(movie.url)){
  detail.url=movie.url[i]
  director.info[[i]]=
    detail.url %>% 
    read_html() %>% 
    html_node(xpath='//*[@id="fullcredits_content"]/table[1]') %>% 
    html_table() %>% 
    .[,1]
}

### first billed cast information
cast.info=list()
cast.url=sub("(.*)\\/.*","\\1",movie.url)
for (i in 1:length(cast.url)){
  detail.url=cast.url[i]
  cast.info[[i]]=
    detail.url %>% 
    read_html() %>% 
    html_node(xpath='//*[@id="titleCast"]/table') %>% 
    html_table() %>% 
    .$X2 %>% 
    .[-1]
}
cast.info=map(cast.info,shrink_cast)

###  rating information
rating.info=
  url %>% 
  read_html() %>% 
  html_nodes("strong") %>% 
  html_text()

### vote information
vote.url=sub("(.*)\\/.*","\\1",movie.url)
vote.info=list()
for (i in 1:length(vote.url)){
  detail.url=vote.url[i]
  vote.info[[i]]=
    detail.url %>% 
    read_html() %>% 
    html_node(xpath='//*[@id="title-overview-widget"]/div[2]/div[2]/div/div[1]/div[1]/a') %>% 
    html_text()
}

```

* For each actor, count how many times he or she starred in a Top 250 Movie. Show the 10 actors/actresses that starred in the most movies among the Top 250. 
```{r}
cast.info %>% 
  unlist %>% 
  table() 

cast.info %>% 
  unlist %>% 
  table() %>% 
  sort(decreasing = T) %>% 
  .[1:10]

```



Show the 10 actors/actresses that starred in movies among the Top 250 with the highest mean rating.
```{r}
index=lapply(cast.info,length) %>% unlist()
expand.rating=expand_x(rating.info,index)
expand.title=expand_x(top.250[,1],index)

### cast versus rating
cast.rating=data.frame(cast=cast.info %>% unlist(),rating=as.numeric(expand.rating),title=expand.title)

cast.rating %>% 
  group_by(cast) %>% 
  summarise(mean.rating=mean(rating)) %>% 
  arrange(desc(mean.rating)) %>% 
  .[1:10,]
```

* For each director, count how many times he or she directed a Top 250 Movie. Show the 10 directors that directed the most movies among the Top 250.
```{r}
director.info %>% 
  unlist %>% 
  table()

director.info %>% 
  unlist %>% 
  table() %>% 
  sort(decreasing = T) %>% 
  .[1:10]
```

*  Show the 10 directors that directed movies among the Top 250 with the highest mean rating.
```{r}
index=lapply(director.info,length) %>% unlist()
expand.rating=expand_x(rating.info,index)
expand.title=expand_x(top.250[,1],index)
director.rating=data.frame(director=director.info %>% unlist(),rating=as.numeric(expand.rating),title=expand.title)

### director versus rating
director.rating %>% 
  group_by(director) %>% 
  summarise(mean.rating=mean(rating)) %>% 
  arrange(desc(mean.rating)) %>% 
  .[1:10,]
  
```

* Show the 10 most frequent Actor-Director collaborations among the Top 250 Movies. What's the average rating for those collaborations?

```{r}
### cast versus director
cast.director=full_join(cast.rating,director.rating,by=c("title"))
cast.director %>% 
  group_by(director,cast) %>% 
  summarise(count=n(),mean.rating=mean(rating.x)) %>% 
  arrange(desc(count)) %>% 
  .[1:10,]
```

* Are ratings influenced by year? In what way? Provide a P-value using linear regression. Are the assumptions of linear regression violated? If so, what's the impact in your P-value estimate?
```{r}
### year versus rating
year.rating=data.frame(year=as.numeric(top.250[,2]),rating=as.numeric(rating.info))
fit=lm(rating~year,year.rating)
summary(fit)
plot(fit)
```

p-value>0.05, we retain the null hypothesis, the ratings are not influenced by year. From the residual plot, we think the assumption of linear regression is violated. 

* Do people vote more often for recent movies? 
```{r}
year.vote=data.frame(vote=vote.info %>% unlist(),title=top.250[,1],year=as.numeric(top.250[,2]))

year.vote$vote=gsub(",","",year.vote$vote)
year.vote$vote=as.numeric(as.character(year.vote$vote))

year.vote %>% 
  arrange(desc(vote)) %>% 
  .[1:10,]


```
I think it reasonable that people vote more for recent movies.


* Provide a P-value using linear regression. Are the assumptions of linear regression violated? If so, what's the impact in your P-value estimate?
```{r}
fit2=lm(vote~year,year.vote)
summary(fit2)
plot(fit2)
```
The assumption of linear regression is violated. The less p-value estimate, the better model can be.


* In light of the previous question, do you think the number of votes influences the rating? 
```{r}
year.vote=year.vote %>% 
  mutate(rating=as.numeric(rating.info))

fit3=lm(rating~vote,year.vote)
summary(fit3)
```

Conclusion:the number of votes influences the rating.

* Create an analysis of variance table for the ratings, considering year, votes and the interaction of year and votes. Explain what the interaction means.
```{r}
fit4=lm(rating~year+vote+year:vote,year.vote)
anova(fit4)
```

the more vote, plus the recent years, correspond to lower ratings.